<!DOCTYPE HTML>
<html lang="en">
  <head>
    <link href="https://fonts.googleapis.com/css2?family=Cinzel+Decorative:wght@700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@700&display=swap" rel="stylesheet">    
    <link href="https://fonts.googleapis.com/css2?family=Abril+Fatface&display=swap" rel="stylesheet">
    <!-- title fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:wght@700&display=swap" rel="stylesheet">
 
    
    <style>
      papertitle {
        font-family: 'Abril Fatface', sans-serif;
        font-size: 1.4em;
        color: #D83F87;
        /* text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.2); */
        letter-spacing: 0.1px;
        display: inline-block;
        transition: transform 0.25s ease-in-out;
      }
    
      papertitle:hover {
        transform: scale(1.05);
      }
      
    </style>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Manu Gaur</title>

    <meta name="author" content="Manu Gaur">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <!-- moose -->
                <p class="name" style="text-align: center;font-family:'Libre Baskerville', sans-serif;">
                  Manu Gaur
                </p>
                <p>
                  Hey there. I am Manu. Iâ€™m a self-taught researcher. I have now spent two wonderful years working with <a href="https://scholar.google.com/citations?user=rJotb-YAAAAJ">Dr. Makarand Tapaswi</a> at the Centre of Visual Information Technology, IIIT Hyderabad. Currently, I am working with <a href="https://yukimasano.github.io/">Dr. Yuki Asano</a> on a new paradigm for vision-language modelling. 
                  Before this, I was a student researcher
                  <!-- an Applied Scientist intern  -->
                  at Amazon's International Machine Learning team, working on GNNs and self-supervised learning. 

                  <!-- I will be joining the Robotics Institute at CMU in Fall 2025 as a graduate student. -->
                  
                  <!-- Hi, I'm a researcher at <a href="https://cvit.iiit.ac.in/">CVIT, IIIT-H</a> working with <a href="https://scholar.google.com/citations?user=rJotb-YAAAAJ">Dr. Makarand Tapaswi</a> and <a href="https://yukimasano.github.io/">Dr. Yuki Asano</a>.  I recently graduated from Delhi Technological University with a major in Applied Physics. I am originally from New Delhi, India. -->
                </p>
                <p>
                  <!-- In a previous life however, I graduated from Delhi Technological University with a major in Applied Physics. However, I became interested in ML during my junior year and started spending most of my time after university watching lectures, reading blogs, engaging in online forums and conducting my own research using Colab GPUs (thanks google!)  -->
                  
                  In a previous life, I graduated from Delhi Technological University. Although I majored in Applied Physics, I became interested in ML during my junior year. Soon after, I was spending most of my time after university watching lectures, reading blogs, engaging in online forums and conducting my own research on free Colab GPUs. Shortly after graduation, I came to IIIT Hyderabad, to learn how to do research from first principles.

                  <!-- In my final semester, I was fortunate to intern as an Applied Scientist at Amazon's International Machine Learning team, working on GNNs and self-supervised learning.  -->


                  <!-- In my final semester, I was fortunate to intern as an Applied Scientist at Amazon's International Machine Learning team, where I worked on GNNs and self-supervised learning for modeling aesthetic compatibility amongst apparel. During my undergraduate studies, I also worked on label-efficient learning for Autism Spectrum classification at the University of Technology, Sydney. -->
                  
                  <!-- more efficient way of doing multimodal modelling 
                  tool use for VLMs
                  RL for abstract shit like storytelling
                  self-supervised learning for videos
                  instilling actual temporal understanding in video language models

                  being a autodidact researcher; I am enjoying studying math
                  
                  I will be joining Robotics Institute this fall.
                  -->
                </p>
                <p>
                Outside of ML , I enjoy physics, history, playing chess, football, and ocassional basketball. Also, I love to travel :)

                </p>
                <p style="text-align:center">
                  <a href="mailto:manugaurwork@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Manu Gaur CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://x.com/gaur_manu">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MGfh5iUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/manugaurdl/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

        <!-- News -->
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li><strong>2024-05:</strong> I joined FAIR, Meta for summer internship with Dr.  <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>,  yayyyyy!</li>
                <li><strong>2024-04:</strong> I received <a href="https://openai.com/blog/superalignment-fast-grants">OpenAI Superalignment Fellowship</a>! Thank you OpenAI!!! Looking forward to the cool works.</li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:'Libre Baskerville', sans-serif;">Research</h2>
                <hr style="border:0; height:1px; background-color:#EEEEEE; margin-top: 10px; margin-bottom: 10px;">

                <p>
                  Broadly, I work on self-supervised learning, vision language models, generative modelling and reinforcement learning. 
                  <!-- My long-term goal is to advance <i>perception</i> and <i>reasoning</i> in next-generation AI systems with limited human supervision. -->
                </p>
                <p>
                  Infants develop visual understanding and common sense reasoning by simply observing and interacting with the world around them.
                  While current systems show remarkable multimodal understanding, progressively squeezing more knowledge into them through supervised learning makes them brittle. 
                  To achieve generalized intelligence, I believe these systems need to independently learn from first principles, either by modeling the underlying structure of data or through trial and error. 
                  </p>
                  <p>
                  Hence, I am interested in self-supervised and reinforcement learning for improving visual understanding, multimodal reasoning, and knowledge acquisition in current systems.

                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:'Libre Baskerville', sans-serif;">Publications</h2>
                <hr style="border:0; height:1px; background-color:#EEEEEE; margin-top: 10px; margin-bottom: 10px;">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


<!-- Paper 1  -->

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/D3.png" alt="PontTuset" width="180" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://katha-ai.github.io/projects/detect-describe-discriminate/" id="MultiMon">
            <papertitle> Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation
            </papertitle>
          </a>
          <br>
          <div style="margin-bottom: 3.2px;margin-top: 4.2px">
            <strong>Manu Gaur</strong>,
            <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
            <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>
          </div>
          <em>ECCV EVAL-FoMo Workshop, 2024</em>
          <br>
          <p style="font-weight: 500"><b>TL;DR:</b> <i>It is easier for MLLMs to select an answer from multiple choices during VQA than to generate it independently</i>.</p>
          <p></p>
          <p>We evaluate MLLMs visual capabilities through self-retrieval within highly similar image pairs, revealing that current models struggle to identify fine-grained visual differences, with open-source models failing to outperform random guess.</p>
        </td>
      </tr>
      
      <mark>
        <tr>
          <td colspan="2" style="height: 30px;"></td>
        </tr></mark>

      <!-- Paper 2-->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/ndlb.png" alt="PontTuset" width="190" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://katha-ai.github.io/projects/no-detail-left-behind/" id="MultiMon">
            <papertitle> No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning
            </papertitle>
          </a>
          <br>
          <div style="margin-bottom: 3.2px;margin-top: 4.2px">
            <strong>Manu Gaur</strong>,
            <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
            <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>
          </div>
          <em>TMLR, 2024</em>
          <br>
          <p style="font-weight: 500"><b>TL;DR:</b> <i>Enhancing visual understanding in MLLMs with a self-supervised verifiable reward</i>.</p>
          <p></p>
          <p>A findings rich paper that systematically improves captioning systems across all fronts: Data, Training, Evaluation. We design: (1) a <i>post-training recipe</i> for self-retrieval finetuning with REINFORCE, and (2) a <i>synthetic framework</i> for visually boosting captioning datasets. Jointly they enable captioners to generate fine-grained, succinct descriptions while reducing hallucinations. Using our training recipe, ClipCap, a 200M param simplication of modern MLLMs, outperforms sota open-source MLLMs on fine-grained visual discrimination.</p>
        </td>
      </tr>


    </table>
  </body>
</html>
