<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Manu Gaur</title>

    <meta name="author" content="Manu Gaur">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Manu Gaur
                </p>
                <p>Hi, I'm a researcher at <a href="https://cvit.iiit.ac.in/">CVIT, IIIT-H</a> working with <a href="https://makarandtapaswi.github.io/">Dr. Makarand Tapaswi</a> and <a href="https://yukimasano.github.io/">Dr. Yuki Asano</a>.  I recently graduated from Delhi Technological University with a major in Applied Physics. I am originally from New Delhi, India.
                </p>
                <p>
                  In my final semester, I was fortunate to intern as an Applied Scientist at Amazon's International Machine Learning team, where I worked on GNNs and self-supervised learning for modeling aesthetic compatibility amongst apparel. During my undergraduate studies, I also worked on label-efficient learning for Autism Spectrum classification at the University of Technology, Sydney.

                </p>
                <p style="text-align:center">
                  <a href="mailto:manugaurwork@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Manu Gaur CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://x.com/gaur_manu">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MGfh5iUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/manugaurdl/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

        <!-- News -->
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li><strong>2024-05:</strong> I joined FAIR, Meta for summer internship with Dr.  <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>,  yayyyyy!</li>
                <li><strong>2024-04:</strong> I received <a href="https://openai.com/blog/superalignment-fast-grants">OpenAI Superalignment Fellowship</a>! Thank you OpenAI!!! Looking forward to the cool works.</li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in self-supervised learning, multimodal models, generative modelling and reinforcement learning. My long-term goal is to advance <i>perception</i> and <i>reasoning</i> in next-generation AI systems with limited human supervision.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- Paper 1  -->


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/D3.png" alt="PontTuset" width="180" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://katha-ai.github.io/projects/detect-describe-discriminate/" id="MultiMon">
            <papertitle> Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation
            </papertitle>
          </a>
          <br>
          <strong>Manu Gaur</strong>,
          <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
          <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>,
          <br>
          <em>ECCV EVAL-FoMo Workshop, 2024</em>
          <br>
          <p></p>
          <p>Given a highly similar image pair, it is easier for an MLLM to identify fine-grained visual differences during VQA evaluation than to independently detect and describe such differences. With such image pairs, we introduce the D<sub>3</sub> benchmark. We use self-retrieval within D<sub>3</sub> to do whitebox evaluation of MLLMs, revealing that current models struggle to independently discern fine-grained visual differences, with open-source models failing to outperform random guess.</p>
        </td>
      </tr>

      <!-- Paper 2-->

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/ndlb.png" alt="PontTuset" width="190" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://katha-ai.github.io/projects/no-detail-left-behind/" id="MultiMon">
            <papertitle> No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning
            </papertitle>
          </a>
          <br>
          <strong>Manu Gaur</strong>,
          <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
          <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>,
          <br>
          <em>TMLR, 2024</em>

          <br>
          <p></p>
          <p>We systematically improve captioning systems all fronts: data, training, evaluation. We introduce <i>Visual Caption Boosting</i> to make image captioning datasets fine-grained and design a training recipe for self-retrieval (SR) fine-tuning with REINFORCE. Jointly they enable captioners to generate more fine-grained descriptions while preserving caption faithfulness. We also introduce TrueMatch, a benchmark that uses SR to evaluate a captioner's ability to capture fine-grained visual differences. Using our training recipe, ClipCap (200M) is able to outperform state-of-the-art open-source MLLMs on TrueMatch while also being SoTA on Image-CoDe.</p>
        </td>
      </tr>





    </table>
  </body>
</html>
