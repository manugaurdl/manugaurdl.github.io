<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Manu Gaur</title>

    <meta name="author" content="Manu Gaur">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Manu Gaur
                </p>
                <p>Hi, I'm a research assistant at <a href="https://cvit.iiit.ac.in/">CVIT, IIIT-H</a> under <a href="https://makarandtapaswi.github.io/">Dr. Makarand Tapaswi</a>. I recently graduated from Delhi Technological University with a major in Applied Physics. I am originally from New Delhi, India.
                </p>
                <p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:manugaurworkj@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="https://x.com/gaur_manu">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MGfh5iUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/manugaurdl/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

        <!-- News -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li><strong>2024-05:</strong> I joined FAIR, Meta for summer internship with Dr.  <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>,  yayyyyy!</li>
                <li><strong>2024-04:</strong> I received <a href="https://openai.com/blog/superalignment-fast-grants">OpenAI Superalignment Fellowship</a>! Thank you OpenAI!!! Looking forward to the cool works.</li>
                <li><strong>2024-04:</strong> Our paper <a href="https://arxiv.org/abs/2401.06209">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a> was accepted at CVPR 2024 as an Oral Paper!</li>
                <li><strong>2024-01:</strong> Our paper <a href="https://arxiv.org/abs/2306.05272">Image Clustering in the Age of Pretrained Models</a> was accepted at ICLR 2024!</li>
                <li><strong>2023-12:</strong> Our papers  were accepted at CPAL 2024!</li>
                <li><strong>2023-09:</strong> I am helping organizing the <a href="https://gkioxari.github.io/Tutorials/iccv2023/">QVCV</a> workshop in ICCV. See you all in Paris!</li>
                <li><strong>2023-09:</strong> Our papers <a href="https://arxiv.org/abs/2306.12105">MultiMon</a> and <a href="https://arxiv.org/abs/2306.01129">CRATE(whitebox-transformer)</a> were accepted at NIPS 2023!</li>
                <li><strong>2023-07:</strong> Our paper <a href="https://arxiv.org/abs/2301.01805">Manifold Linearizing and Clustering</a> was accepted at ICCV 2023!</li>
                <li><strong>2023-05:</strong> I graduated from UC Berkeley with triple degree Applied Math (Honor), Statistics (Honor) and Computer Science (No honor, because I didn't want to take 16b and 61c too early, but I published quite some interesting work so yay)!!!</li>
                <li><strong>2023-04:</strong> I will be a CS PhD student in NYU Courant advised by <a href="https://yann.lecun.com/" target="_blank">Professor Yann LeCun</a> and <a href="https://sainingxie.com/" target="_blank">Professor Saining Xie</a>. Looking forward to working with Yann and Saining in New York!</li>
                <li><strong>2023-01:</strong> Our paper <a href="https://arxiv.org/abs/2202.05411">incremental-CTRL</a> was accepted at ICLR 2023!</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in self-supervised learning, multimodal models, generative modelling and reinforcement learning. My long term goal is ...
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- Paper 1  -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/cambrian.png" alt="PontTuset" width="160" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://www.arxiv.org/abs/2409.03025" id="MultiMon">
            <papertitle> No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning
            </papertitle>
          </a>
          <br>
          <strong>Manu Gaur*</strong>,
          <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
          <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>,
          <br>
          <em>Under Review </em>

          <br>
          <p></p>
          <p>MLLMs generate generic captions. We outperform MLLMs 30x size.</p>
        </td>
      </tr>

<!-- Paper 2-->


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/cambrian.png" alt="PontTuset" width="160" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://www.arxiv.org/abs/2409.03025" id="MultiMon">
            <papertitle> D3: Evaluating MLLM's capacity for Finegrained Visual Discrimination through Self-Retrieval
            </papertitle>
          </a>
          <br>
          <strong>Manu Gaur*</strong>,
          <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
          <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>,
          <br>
          <em>ECCV EVAL-FoMo Workshop, 2024</em>
          <br>
          <p></p>
          <p>Closed source MLLMs struggle in capturing finegrained visual differences, with open-source models failing to outperform random guess.</p>
        </td>
      </tr>


    </table>
  </body>
</html>
