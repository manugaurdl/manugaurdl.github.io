<!DOCTYPE HTML>
<html lang="en">
  <head>
    <link href="https://fonts.googleapis.com/css2?family=Cinzel+Decorative:wght@700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@700&display=swap" rel="stylesheet">    
    <link href="https://fonts.googleapis.com/css2?family=Abril+Fatface&display=swap" rel="stylesheet">
    <!-- title fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:wght@700&display=swap" rel="stylesheet">
 
    
    <style>
      papertitle {
        font-family: 'Abril Fatface', sans-serif;
        font-size: 1.4em;
        color: #D83F87;
        /* text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.2); */
        letter-spacing: 0.1px;
        display: inline-block;
        transition: transform 0.25s ease-in-out;
      }
    
      papertitle:hover {
        transform: scale(1.05);
      }
      
    </style>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Manu Gaur</title>

    <meta name="author" content="Manu Gaur">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <!-- moose -->
                <p class="name" style="text-align: center;font-family:'Libre Baskerville', sans-serif;">
                  Manu Gaur
                </p>
                <p>
                  
                  
                  Hi, I'm a researcher at <a href="https://cvit.iiit.ac.in/">CVIT, IIIT-H</a> working with <a href="https://makarandtapaswi.github.io/">Dr. Makarand Tapaswi</a> and <a href="https://yukimasano.github.io/">Dr. Yuki Asano</a>.  I recently graduated from Delhi Technological University with a major in Applied Physics. I am originally from New Delhi, India.
                </p>
                <p>
                  In my final semester, I was fortunate to intern as an Applied Scientist at Amazon's International Machine Learning team, where I worked on GNNs and self-supervised learning for modeling aesthetic compatibility amongst apparel. During my undergraduate studies, I also worked on label-efficient learning for Autism Spectrum classification at the University of Technology, Sydney.
                  <!-- more efficient way of doing multimodal modelling 
                  tool use for VLMs
                  RL for abstract shit like storytelling
                  self-supervised learning for videos
                  instilling actual temporal understanding in video language models

                  being a autodidact researcher; I am enjoying studying math
                  
                  I will be joining Robotics Institute this fall.
                  -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:manugaurwork@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Manu Gaur CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://x.com/gaur_manu">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MGfh5iUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/manugaurdl/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

        <!-- News -->
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li><strong>2024-05:</strong> I joined FAIR, Meta for summer internship with Dr.  <a href = "https://liuzhuang13.github.io/">Zhuang Liu</a>,  yayyyyy!</li>
                <li><strong>2024-04:</strong> I received <a href="https://openai.com/blog/superalignment-fast-grants">OpenAI Superalignment Fellowship</a>! Thank you OpenAI!!! Looking forward to the cool works.</li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:'Libre Baskerville', sans-serif;">Research</h2>
                <hr style="border:0; height:1px; background-color:#EEEEEE; margin-top: 10px; margin-bottom: 10px;">

                <p>
                  I am interested in self-supervised learning, multimodal models, generative modelling and reinforcement learning. My long-term goal is to advance <i>perception</i> and <i>reasoning</i> in next-generation AI systems with limited human supervision.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:'Libre Baskerville', sans-serif;">Publications</h2>
                <hr style="border:0; height:1px; background-color:#EEEEEE; margin-top: 10px; margin-bottom: 10px;">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


<!-- Paper 1  -->

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/D3.png" alt="PontTuset" width="180" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://katha-ai.github.io/projects/detect-describe-discriminate/" id="MultiMon">
            <papertitle> Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation
            </papertitle>
          </a>
          <br>
          <div style="margin-bottom: 3.2px;margin-top: 4.2px">
            <strong>Manu Gaur</strong>,
            <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
            <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>
          </div>
          <em>ECCV EVAL-FoMo Workshop, 2024</em>
          <br>
          <p style="font-weight: 500"><b>TL;DR:</b> <i>It is easier for MLLMs to select an answer from multiple choices than to generate it independently</i>.</p>
          <p></p>
          <p>We evaluate MLLMs visual capabilities through self-retrieval within highly similar image pairs. Our benchmark reveals that current models struggle to identify fine-grained visual differences, with open-source models failing to outperform random guess.</p>
        </td>
      </tr>
      
      <mark>
        <tr>
          <td colspan="2" style="height: 30px;"></td>
        </tr></mark>

      <!-- Paper 2-->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/ndlb.png" alt="PontTuset" width="190" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://katha-ai.github.io/projects/no-detail-left-behind/" id="MultiMon">
            <papertitle> No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning
            </papertitle>
          </a>
          <br>
          <div style="margin-bottom: 3.2px;margin-top: 4.2px">
            <strong>Manu Gaur</strong>,
            <a href="https://scholar.google.com/citations?user=CHFzTuQAAAAJ&hl=en" target="_blank">Darshan Singh</a>,
            <a href="https://makarandtapaswi.github.io/" target="_blank">Makarand Tapaswi</a>
          </div>
          <em>TMLR, 2024</em>
          <br>
          <p style="font-weight: 500"><b>TL;DR:</b> <i>Enhancing visual understanding in MLLMs with a self-supervised verifiable reward</i>.</p>
          <p></p>
          <p>A findings rich paper that systematically improves captioning systems across all fronts: Data, Training, Evaluation. We design: (1) a <i>post-training recipe</i> for self-retrieval finetuning with REINFORCE, and (2) a <i>synthetic framework</i> for visually boosting captioning datasets. Jointly they enable captioners to generate fine-grained, succinct descriptions while reducing hallucinations. Using our training recipe, ClipCap, a 200M param simplication of modern MLLMs, outperforms sota open-source MLLMs on fine-grained visual discrimination.</p>
        </td>
      </tr>


    </table>
  </body>
</html>
